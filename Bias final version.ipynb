{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Understanding and Correcting Bias in Modern Machine Learning"}, {"metadata": {}, "cell_type": "markdown", "source": "\n\nA few years ago, I was made aware of a new software product that corrected bias in machine learning models.  My initial response?  \"What the heck is bias in machine learning models?\"  I seriously had no idea.  Of course, there are different types of bias when doing Data Science, but I didn't understand what problem the product was solving.  Since then, I have seen the term \"machine learning model bias\" used repeatedly.  I decided I couldn't stay in the dark forever, so I set out to do a little research on the topic.\n\nAfter doing some research, I think I understand what most people mean when they say a machine learning model has \"bias\".  I say most because many people use the term and have no idea what they are talking about.  \"Bias\" is not a new problem, just a new name (or at least new to me).  I don't like that we call the issue at hand \"bias\".  The term \"bias\" is too general and applies to multiple problems in multiple contexts.  For example, you can have confirmation bias when you misinterpret data.  Also, I learned many years ago in statistics that bias occurs when a parameter estimate differs from the population parameter.  Terms like heteroskedasticity are hard to say, but there is no doubt what I mean when I use that specific word.  Overusing simple phrases like \"bias\" just confuses everyone.\n \n\nGenerally, when someone talks about \"bias\" in a machine learning model, it is usually in the context of gender, racial or ethnic discrimination.  If a machine learning model reflects historical discrimination against a cohort of people, it is  \"biased\".  It isn't that machine learning is prejudiced against anyone, but the people/organization/social constructs who generated the historical data were.  This means that a machine learning model based on a history of excluding specific cohorts will replicate that behavior.\n\nHere in North America, the statement above edges into a bit of controversy.  The idea of historical discrimination and how it impacts life today is a  sensitive topic.  I'll do my best to steer clear of saying anything controversial and attempt to focus on the use cases from an objective point of view.  \n\nTo stay neutral and non-political, I will examine this issue from a broader perspective.  As I mentioned, the term \"bias\" is somewhat new to me, but the general idea that the historical record may not meet your current objective isn't.  That's the core issue.  Machine learning models can be problematic when organizations seek to achieve new goals that differ from the past.  For example, in the case of gender discrimination in tech hiring, if the historical record of hires lacks females, a machine learning model may conclude that females are not worthy candidates and predict them as less hireable.   The model does not know that women were denied employment in tech roles based on their gender, not their ability to do the job.  Machine Learning is a matter-of-fact kind of thing.  It can not automatically adjust itself to correct the fact that previous managers failed to hire qualified females.  Note, this is only a problem if the organization wants more females in its workforce.  If it is happy hiring men as it did in the past, a machine learning model using the historical data will be acceptable.  \n\nSo, this article is about handling situations when your historical repository does not reflect how you want to do business today. \n\nGenerally, there are three ways you can adjust your models when the current objectives differ from the historical data.  One, you can monkey With the scoring data.  Two, you can monkey with the modeling sample.  Or, three, you can monkey with the output data.\n\nNote that neither of these three fixes is perfect.  Each is just a way to get actionable results in a less-than-ideal situation.  Which method makes the most sense depends on the context of the problem.\n\nI will be looking at two different use cases to highlight the methods of correcting \"bias\".  The first is new market expansion.  This is a classic example of when the historical record does not reflect the current objectives of the business.  The second is a discrimination example, where a particular group has had minimal access historically.  \n\n\n\n\nNote that all of the data used in this article are 100% fake.  I created the data, and 100% own it. "}, {"metadata": {}, "cell_type": "markdown", "source": "## Table of contents"}, {"metadata": {}, "cell_type": "markdown", "source": "1. [Getting Setup](#10)<br>\n    1.1 [Install Relevant Libraries](#11)<br>\n\n2. [New Market Expansion](#20)<br>\n    2.1 [Import Data From GitHub](#21)<br>\n    2.2 [Examining the Data](#22)<br>\n    2.3 [ Machine Learning Model on the Raw Data](#23)<br>\n    2.4 [ Monkey with the scoring data](#24)<br>\n    2.5 [ Monkey with the Sample](#25)<br>\n    \n3. [Historical Discrimination](#30)<br>\n    3.1 [Pull in the data from GitHub](#31)<br>\n    3.2 [Examine the results](#32)<br>\n    3.3 [Build a Machine Learning Model to predict the best prospects](#33)<br>\n    3.4 [Monkey with the Output Data](#34)<br>\n    \n4. [Conclusion](#40)<br>\n "}, {"metadata": {}, "cell_type": "markdown", "source": "## 1.0 Getting Setup <a id=\"10\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": " #### 1.1 Import relevant libraries <a id=\"11\"></a>"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "import numpy.dual as dual\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn import metrics\n\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\n\nfrom IPython.display import Image\nfrom IPython.core.display import HTML \n\n\n#Un-Comment these options if you want to exapand the number of rows and columns of you see visually in the notebook.\n#pd.set_option('display.max_columns', None)\n#pd.set_option('display.max_rows', None)", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "##  2.0 New Market Expansion <a id=\"20\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "Barney BA and Associates (BBAA) operates a successful carpet cleaning business.  Their business model is pretty simple.  Their marketing budget is limited, and they have found great success by mailing coupons to prospects in the mail.  Consumers open their mailbox, find a coupon, realize their carpets are dirty, and call BBAA for an appointment.   Again, BBAA has a limited marketing budget, so they can not mail coupons to every household in their target market.  They use a machine learning model to maximize their marketing dollars to predict which consumers are most likely to need a carpet cleaning.  After constructing their machine learning model, they only mail to the top 30% of the prospects.  Focusing on the 30% most likely to buy enables them to maximize their marketing expenditures.\n\nKarma, Yuba, and Yarnaby are three suburban communities in a larger metro area.  BBAA focuses on Yuba and Karma, but they are looking to expand into Yarnaby.  Note that BBAA has not spent any marketing dollars in Yarnaby or focused on doing business in Yarnaby.  They do have a few customers in Yarnaby, however.  These customers come from referrals or customers who lived in either Karma or Yuba and moved to Yarnaby as a BBAA customers.  \n\nConsistent with their business model, BBAA would like machine learning to guide their market expansion."}, {"metadata": {}, "cell_type": "markdown", "source": "##### 2.1 Import Data From GitHub <a id=\"21\"></a>"}, {"metadata": {}, "cell_type": "code", "source": "!rm markets.csv\n!wget https://raw.githubusercontent.com/shadgriffin/ml_bias/main/markets.csv\n    \ndf = pd.read_csv(\"markets.csv\", sep=\",\", header=0)\n\n", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "--2022-01-07 22:31:32--  https://raw.githubusercontent.com/shadgriffin/ml_bias/main/markets.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5809035 (5.5M) [text/plain]\nSaving to: \u2018markets.csv\u2019\n\nmarkets.csv         100%[===================>]   5.54M  --.-KB/s    in 0.03s   \n\n2022-01-07 22:31:32 (176 MB/s) - \u2018markets.csv\u2019 saved [5809035/5809035]\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "##### 2.2 Examining the Data <a id=\"22\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "Let's take a look at that the data "}, {"metadata": {}, "cell_type": "code", "source": "df.head()", "execution_count": 3, "outputs": [{"output_type": "execute_result", "execution_count": 3, "data": {"text/plain": "    CUST_ID  THEPREDICTOR   MARKET  BUY\n0  10000275    -98.151992  Yarnaby    1\n1  10000617    -98.991996  Yarnaby    1\n2  10000716    -99.051996  Yarnaby    1\n3  10000823    -97.779991  Yarnaby    1\n4  10001492    -97.854991  Yarnaby    1", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CUST_ID</th>\n      <th>THEPREDICTOR</th>\n      <th>MARKET</th>\n      <th>BUY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10000275</td>\n      <td>-98.151992</td>\n      <td>Yarnaby</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10000617</td>\n      <td>-98.991996</td>\n      <td>Yarnaby</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10000716</td>\n      <td>-99.051996</td>\n      <td>Yarnaby</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10000823</td>\n      <td>-97.779991</td>\n      <td>Yarnaby</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10001492</td>\n      <td>-97.854991</td>\n      <td>Yarnaby</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "The dataset is pretty simple.  Again, it is 100% fake. \n\nCUST_ID -- Unique Identifier of the prospect\n\nTHEPREDICTOR -- This is the primary independent variable of the model.\n\nMARKET -- The market of the prospect.  Can be Yuba, Yarnaby, or Karma\n\nBUY -- The dependent variable.  A '1' means the prospect purchased carpet cleaning in the past, a '0' means they did not."}, {"metadata": {}, "cell_type": "markdown", "source": "The three markets have about the same number of prospects."}, {"metadata": {}, "cell_type": "code", "source": "df_count = pd.DataFrame(df.groupby(['MARKET'])['BUY'].count())\ndf_count", "execution_count": 4, "outputs": [{"output_type": "execute_result", "execution_count": 4, "data": {"text/plain": "           BUY\nMARKET        \nKarma    54975\nYarnaby  53006\nYuba     54908", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BUY</th>\n    </tr>\n    <tr>\n      <th>MARKET</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Karma</th>\n      <td>54975</td>\n    </tr>\n    <tr>\n      <th>Yarnaby</th>\n      <td>53006</td>\n    </tr>\n    <tr>\n      <th>Yuba</th>\n      <td>54908</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "As expected, the conversion rate and the number of conversions in Yarnaby are much lower than in other markets.\n\nFewer conversions in Yarnaby is because BBAA has never focused on doing business there.   If historically they have never tried to sell products in Yarnaby, they will not have many sales there.  Building a model on historical data will not facilitate selling into a new market."}, {"metadata": {}, "cell_type": "code", "source": "df_count = pd.DataFrame(df.groupby(['MARKET'])['BUY'].mean())\ndf_count", "execution_count": 5, "outputs": [{"output_type": "execute_result", "execution_count": 5, "data": {"text/plain": "              BUY\nMARKET           \nKarma    0.041237\nYarnaby  0.000434\nYuba     0.040959", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BUY</th>\n    </tr>\n    <tr>\n      <th>MARKET</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Karma</th>\n      <td>0.041237</td>\n    </tr>\n    <tr>\n      <th>Yarnaby</th>\n      <td>0.000434</td>\n    </tr>\n    <tr>\n      <th>Yuba</th>\n      <td>0.040959</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "df_count = pd.DataFrame(df.groupby(['MARKET'])['BUY'].sum())\ndf_count", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "          BUY\nMARKET       \nKarma    2267\nYarnaby    23\nYuba     2249", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BUY</th>\n    </tr>\n    <tr>\n      <th>MARKET</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Karma</th>\n      <td>2267</td>\n    </tr>\n    <tr>\n      <th>Yarnaby</th>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>Yuba</th>\n      <td>2249</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "So, we have three markets.  Two markets are mature, and one is new, with few customers.  Let's see our prospect list if we build a machine learning model with the existing data.  In the next section, we will create a machine learning model from the historical record and identify the top 30% of prospects that are likely to buy carpet cleaning.  Once we have done this, we can mail them coupons in the mail."}, {"metadata": {}, "cell_type": "markdown", "source": "##### 2.3 Machine Learning Model on the Raw Data <a id=\"23\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "Create a dummy variable that reflects identifies prospects in Yarnaby.  We will use this as a dependent variable in the model."}, {"metadata": {}, "cell_type": "code", "source": "df['YARNABY'] = np.where(((df.MARKET == 'Yarnaby')), 1, 0)", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Define the Independent and dependent variables."}, {"metadata": {}, "cell_type": "code", "source": "X=pd.DataFrame(df[['THEPREDICTOR','YARNABY']])\ny=pd.DataFrame(df['BUY'])", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Define the model"}, {"metadata": {}, "cell_type": "code", "source": "xgb21 = XGBClassifier(objective = 'binary:logistic',use_label_encoder=False);", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Fit the model."}, {"metadata": {}, "cell_type": "code", "source": "xgb21.fit(X,y,eval_metric='auc')", "execution_count": 10, "outputs": [{"output_type": "execute_result", "execution_count": 10, "data": {"text/plain": "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=100, n_jobs=56, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', use_label_encoder=False,\n              validate_parameters=1, verbosity=None)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Score the data and evaluate the accuracy."}, {"metadata": {}, "cell_type": "code", "source": "phat= xgb21.predict_proba(X);\nyhat= xgb21.predict(X);\ndf_phat=pd.DataFrame(phat)\ndf_phat=df_phat.rename(columns={0: \"P_NOBUY\", 1:\"P_BUY\"})\ndf_yhat=pd.DataFrame(yhat)\nprint(\"\\nModel Report\")\nprint(\"Accuracy : %.4g\" % metrics.accuracy_score(y, yhat))\nprint(\"AUC Score: %f\" % metrics.roc_auc_score(y, yhat))    ", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "\nModel Report\nAccuracy : 0.9906\nAUC Score: 0.898325\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Append the scores to the original data"}, {"metadata": {}, "cell_type": "code", "source": "dfx = pd.concat([df, df_phat], axis=1)\n\n", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Assign records to deciles.  The firm will send coupons to the top 30%."}, {"metadata": {}, "cell_type": "code", "source": "dfx['bingbong'] = (np.random.randint(0, 100, df.shape[0]))/100000000000000000\n\ndfx['P_BUY']=(dfx['P_BUY']+dfx['bingbong'])#*dfx['ADJUSTER']\n\n#Create deciles based on P_BUY\ndfx['DECILE'] = pd.qcut(dfx['P_BUY'], 10, labels=np.arange(100, 0, -10))\n", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Look at the accuracy of the model based on the deciles.  (10 is the highest, 100 is the lowest.)\nThe bulk or Buyers are in the top deciles, which is good.  Also, the distribution of prospects in the expansion market is weighted towards the lower deciles. "}, {"metadata": {}, "cell_type": "code", "source": "df_sum = pd.DataFrame(dfx.groupby(['DECILE'])['BUY'].sum())\n\ndf_count = pd.DataFrame(dfx.groupby(['DECILE'])['BUY'].count())\ndf_mean = pd.DataFrame(dfx.groupby(['DECILE'])['BUY'].mean())\ndf_phat = pd.DataFrame(dfx.groupby(['DECILE'])['P_BUY'].mean())\ndfx['Yarnaby'] = np.where(((dfx.MARKET == 'Yarnaby')), 1, 0)\ndf_yarnaby = pd.DataFrame(dfx.groupby(['DECILE'])['Yarnaby'].sum())\n\ndfx.head()\n\n\ndf_out = pd.concat([df_mean, df_count,df_sum,df_yarnaby,df_phat], axis=1)\n\ndf_out.columns = [['Historical Conversion Rate', 'Total Prospects', 'Historical Conversions','Prospects in Yarnaby','Predicted Conversion Rate']]\ndf_out", "execution_count": 14, "outputs": [{"output_type": "execute_result", "execution_count": 14, "data": {"text/plain": "       Historical Conversion Rate Total Prospects Historical Conversions  \\\nDECILE                                                                     \n100                      0.000000           16290                      0   \n90                       0.000000           16289                      0   \n80                       0.000000           16288                      0   \n70                       0.000614           16292                     10   \n60                       0.001781           16286                     29   \n50                       0.003745           16288                     61   \n40                       0.006200           16290                    101   \n30                       0.009944           16291                    162   \n20                       0.014243           16289                    232   \n10                       0.242171           16286                   3944   \n\n       Prospects in Yarnaby Predicted Conversion Rate  \nDECILE                                                 \n100                   16290                  0.000019  \n90                    16289                  0.000029  \n80                    16288                  0.000512  \n70                     2941                  0.002875  \n60                      394                  0.004655  \n50                      294                  0.005950  \n40                      200                  0.007212  \n30                      202                  0.009072  \n20                       27                  0.011452  \n10                       81                  0.236942  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>Historical Conversion Rate</th>\n      <th>Total Prospects</th>\n      <th>Historical Conversions</th>\n      <th>Prospects in Yarnaby</th>\n      <th>Predicted Conversion Rate</th>\n    </tr>\n    <tr>\n      <th>DECILE</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>100</th>\n      <td>0.000000</td>\n      <td>16290</td>\n      <td>0</td>\n      <td>16290</td>\n      <td>0.000019</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>0.000000</td>\n      <td>16289</td>\n      <td>0</td>\n      <td>16289</td>\n      <td>0.000029</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>0.000000</td>\n      <td>16288</td>\n      <td>0</td>\n      <td>16288</td>\n      <td>0.000512</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>0.000614</td>\n      <td>16292</td>\n      <td>10</td>\n      <td>2941</td>\n      <td>0.002875</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>0.001781</td>\n      <td>16286</td>\n      <td>29</td>\n      <td>394</td>\n      <td>0.004655</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>0.003745</td>\n      <td>16288</td>\n      <td>61</td>\n      <td>294</td>\n      <td>0.005950</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.006200</td>\n      <td>16290</td>\n      <td>101</td>\n      <td>200</td>\n      <td>0.007212</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.009944</td>\n      <td>16291</td>\n      <td>162</td>\n      <td>202</td>\n      <td>0.009072</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.014243</td>\n      <td>16289</td>\n      <td>232</td>\n      <td>27</td>\n      <td>0.011452</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.242171</td>\n      <td>16286</td>\n      <td>3944</td>\n      <td>81</td>\n      <td>0.236942</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Identify the top 30% and examine the distribution across markets.\n"}, {"metadata": {}, "cell_type": "code", "source": "dfx['DECILE']=dfx['DECILE'].astype(int)", "execution_count": 15, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_x=dfx[dfx['DECILE']<=30]", "execution_count": 16, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_count = pd.DataFrame(df_x.groupby(['MARKET'])['BUY'].count())", "execution_count": 17, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_count", "execution_count": 18, "outputs": [{"output_type": "execute_result", "execution_count": 18, "data": {"text/plain": "           BUY\nMARKET        \nKarma    24261\nYarnaby    310\nYuba     24295", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BUY</th>\n    </tr>\n    <tr>\n      <th>MARKET</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Karma</th>\n      <td>24261</td>\n    </tr>\n    <tr>\n      <th>Yarnaby</th>\n      <td>310</td>\n    </tr>\n    <tr>\n      <th>Yuba</th>\n      <td>24295</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "See the problem?  The historical data has almost no sales from Yarnaby, and without doing something, they never will.  It becomes a \"self-fulfilling prophecy\".  If you use historical sales to predict future sales, you will never have many sales in a new market.  \n\nUnless we do something here, we will never be successful in our new market.\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "##### 2.4 Monkey  with the scoring data <a id=\"24\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "The most straightforward thing to do here is to apply the model to Yarnaby but pretend that the prospects are either in Yuba or Karma.  That is, recode the \"YARNABY\" variable so that all records (even those in Yarnaby) have a value of 0.  Changing the market of the Yarnaby records means that when you apply the model, we will score every record as if it was in Yuba or Karma, even if it isn't."}, {"metadata": {}, "cell_type": "code", "source": "df['YARNABY']=0", "execution_count": 19, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Define the independent and dependent variable.\nX=pd.DataFrame(df[['THEPREDICTOR','YARNABY']])\ny=pd.DataFrame(df['BUY'])\n\n#Score the data \nphat= xgb21.predict_proba(X);\n\ndf_phat=pd.DataFrame(phat)\ndf_phat=df_phat.rename(columns={0: \"P_NOBUY\", 1:\"P_BUY\"})\n\n#Append the scores to the original data\n\ndfx = pd.concat([df, df_phat], axis=1)\n#Assign records to deciles. The firm will send coupons to the top 30%.\ndfx['bingbong'] = (np.random.randint(0, 100, df.shape[0]))/100000000000000000\n\ndfx['P_BUY']=(dfx['P_BUY']+dfx['bingbong'])\n\n#Identify the top 30% and examine the distribution across markets.\ndfx['DECILE'] = pd.qcut(dfx['P_BUY'], 10, labels=np.arange(100, 0, -10))\ndfx['DECILE']=dfx['DECILE'].astype(int)\ndf_x=dfx[dfx['DECILE']<=30]\ndf_count = pd.DataFrame(df_x.groupby(['MARKET'])['BUY'].count())\ndf_count", "execution_count": 20, "outputs": [{"output_type": "execute_result", "execution_count": 20, "data": {"text/plain": "           BUY\nMARKET        \nKarma    16893\nYarnaby  15072\nYuba     16902", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BUY</th>\n    </tr>\n    <tr>\n      <th>MARKET</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Karma</th>\n      <td>16893</td>\n    </tr>\n    <tr>\n      <th>Yarnaby</th>\n      <td>15072</td>\n    </tr>\n    <tr>\n      <th>Yuba</th>\n      <td>16902</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "The results of Monkeying with the scoring Data appear above.  Note there are now a significant number of prospects in Yarnaby.  \n\nA word of caution, however.  This method assumes that all market factors exist in one variable and that by changing the variable, you can remove all of the market effects.  In practice, this is rarely the case.  Data used as inputs in a machine learning model are almost always correlated.  In classical regression methods, we call this collinearity or multi-collinearity.  So, even if you change the market for Yarnaby prospects, there will likely be some lingering market effect in other variables.  A good example is household income.  Income and market are typically collinear, given that households with similar incomes tend to live by one another. \n\nAlso, it is essential to point out that the viability of the Yarnaby market is unknown.  We are making a pretty big assumption that it is as viable as the other markets, but BBAA won't know that until they start selling in the market.\n\nThis use case is what I would refer to as a batch use case.  That is, we are scoring the complete set of prospects and then taking action by focusing on the top 30%.  Sometimes your deployment would be what I would call real-time.  That is, you will need to take action on a single prospect without the benefit of aggregating a larger cohort.  For example, you could be deploying this model in an application that scores prospects one at a time instead of thousands at a time.  \n\nMonkeying with the scoring data lends itself to a real-time deployment, given that the scores for all three markets are comparable.  In other words, the average score or predicted value will be about the same across all three markets.  Or, a good prospect will have roughly the same value, regardless of their market.\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "##### 2.5 Monkey  with the Sample<a id=\"25\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "Another way to address this problem is to resample the original data before building any machine learning model. Again, this is not perfect but does make sense in some situations. Currently, the historical purchase rate for Yarnaby is substantially lower than the other two markets. If we resample Yarnaby to have a similar historical purchase rate, a model should reflect this when applied to prospect data."}, {"metadata": {}, "cell_type": "markdown", "source": "Read in the csv."}, {"metadata": {}, "cell_type": "code", "source": "df = pd.read_csv(\"markets.csv\", sep=\",\", header=0)\n", "execution_count": 21, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Create a dummy variable for records in Yarnaby"}, {"metadata": {}, "cell_type": "code", "source": "df['YARNABY'] = np.where(((df.MARKET == 'Yarnaby')), 1, 0)", "execution_count": 22, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Put the records in random order"}, {"metadata": {}, "cell_type": "code", "source": "df['loopy'] = (np.random.randint(0, 1000, df.shape[0]))/1000\ndf=df.sort_values(by=['loopy'])\ndf=df.reset_index(drop=True)", "execution_count": 23, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Create a separate data frame for each market based on the \"Buy\" variable."}, {"metadata": {}, "cell_type": "code", "source": "dfyarnb=df[(df['MARKET']=='Yarnaby') & (df['BUY']==1)]\ndfyarnn=df[(df['MARKET']=='Yarnaby') & (df['BUY']==0)]\ndfyubab=df[(df['MARKET']=='Yuba') & (df['BUY']==1)]\ndfyuban=df[(df['MARKET']=='Yuba') & (df['BUY']==0)]\ndfkarmab=df[(df['MARKET']=='Karma') & (df['BUY']==1)]\ndfkarman=df[(df['MARKET']=='Karma') & (df['BUY']==0)]", "execution_count": 24, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "For each Buyer in each market, randomly select 20 non-buyers.  The conversion rate will be the same for each market in our new data frame."}, {"metadata": {}, "cell_type": "code", "source": "a=(dfyarnb['CUST_ID'].count())*20\nb=(dfyubab['CUST_ID'].count())*20\nc=(dfkarmab['CUST_ID'].count())*20\n\n\ndfyarnn=dfyarnn.sort_values(by=['loopy'])\n\n\ndfyarnn=dfyarnn.head(a)\ndfyuban=dfyuban.head(b)\ndfkarman=dfkarman.head(c)", "execution_count": 25, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Concatenate the adjusted data frames into a complete data frame."}, {"metadata": {}, "cell_type": "code", "source": "dfz = pd.concat([dfyarnn, dfyarnb, dfyuban, dfyubab, dfkarman, dfkarmab])", "execution_count": 26, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Reset the Indexes"}, {"metadata": {}, "cell_type": "code", "source": "dfz=dfz.sort_values(by=['CUST_ID'])\ndfz=dfz.reset_index(drop=True)\n\ndf=df.sort_values(by=['CUST_ID'])\ndf=df.reset_index(drop=True)", "execution_count": 27, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The original dataframe has 162,889 records."}, {"metadata": {}, "cell_type": "code", "source": "df.shape", "execution_count": 28, "outputs": [{"output_type": "execute_result", "execution_count": 28, "data": {"text/plain": "(162889, 6)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "The new data frame with a consistent conversion rate in each market has 95,319 records."}, {"metadata": {}, "cell_type": "code", "source": "dfz.shape", "execution_count": 29, "outputs": [{"output_type": "execute_result", "execution_count": 29, "data": {"text/plain": "(95319, 6)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Note, now all markets have the same conversion rate."}, {"metadata": {}, "cell_type": "code", "source": "zss = pd.DataFrame(dfz.groupby(['MARKET'])['BUY'].mean())\nzss", "execution_count": 30, "outputs": [{"output_type": "execute_result", "execution_count": 30, "data": {"text/plain": "              BUY\nMARKET           \nKarma    0.047619\nYarnaby  0.047619\nYuba     0.047619", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BUY</th>\n    </tr>\n    <tr>\n      <th>MARKET</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Karma</th>\n      <td>0.047619</td>\n    </tr>\n    <tr>\n      <th>Yarnaby</th>\n      <td>0.047619</td>\n    </tr>\n    <tr>\n      <th>Yuba</th>\n      <td>0.047619</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Define the dependent and independent variables for the modeling and scoring data set.  Note that we will build the model on the data frame with the adjusted sample, and we will apply that model to the complete record data set."}, {"metadata": {}, "cell_type": "code", "source": "#Define the scoring data set\nX=pd.DataFrame(df[['THEPREDICTOR','YARNABY']])\ny=pd.DataFrame(df['BUY'])\n#Define the modeling data set\nX1=pd.DataFrame(dfz[['THEPREDICTOR','YARNABY']])\ny1=pd.DataFrame(dfz['BUY'])", "execution_count": 31, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Define the Model\nxgb21 = XGBClassifier(objective = 'binary:logistic',use_label_encoder=False);\n#Fit the Model to the adjusted sample\nxgb21.fit(X1,y1,eval_metric='auc')\n#apply or score the model against the original dataframe\nphat= xgb21.predict_proba(X);\n#write to a dataframe\ndf_phat=pd.DataFrame(phat)\n#Rename the columns in the scored data set\ndf_phat=df_phat.rename(columns={0: \"P_NOBUY\", 1:\"P_BUY\"})\n#concatenate the scored data to the original non-augmented data frame.\ndfx = pd.concat([df, df_phat], axis=1)\n#Examine the results\ndfx.head()", "execution_count": 32, "outputs": [{"output_type": "execute_result", "execution_count": 32, "data": {"text/plain": "    CUST_ID  THEPREDICTOR   MARKET  BUY  YARNABY  loopy   P_NOBUY     P_BUY\n0  10000000    -98.361993    Karma    0        0  0.539  0.994262  0.005738\n1  10000001    -91.149964     Yuba    0        0  0.763  0.992324  0.007676\n2  10000002    -99.321997     Yuba    0        0  0.340  0.990993  0.009007\n3  10000003    -99.597998  Yarnaby    0        1  0.862  0.854623  0.145377\n4  10000004    -99.408998    Karma    0        0  0.136  0.990805  0.009195", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CUST_ID</th>\n      <th>THEPREDICTOR</th>\n      <th>MARKET</th>\n      <th>BUY</th>\n      <th>YARNABY</th>\n      <th>loopy</th>\n      <th>P_NOBUY</th>\n      <th>P_BUY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10000000</td>\n      <td>-98.361993</td>\n      <td>Karma</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.539</td>\n      <td>0.994262</td>\n      <td>0.005738</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10000001</td>\n      <td>-91.149964</td>\n      <td>Yuba</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.763</td>\n      <td>0.992324</td>\n      <td>0.007676</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10000002</td>\n      <td>-99.321997</td>\n      <td>Yuba</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.340</td>\n      <td>0.990993</td>\n      <td>0.009007</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10000003</td>\n      <td>-99.597998</td>\n      <td>Yarnaby</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.862</td>\n      <td>0.854623</td>\n      <td>0.145377</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10000004</td>\n      <td>-99.408998</td>\n      <td>Karma</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.136</td>\n      <td>0.990805</td>\n      <td>0.009195</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "#Evaluate the accuracy of the model built on the augmented data on the original data\nyhat= xgb21.predict(X);\n\n\ndf_yhat=pd.DataFrame(yhat)\nprint(\"\\nModel Report\")\nprint(\"Accuracy : %.4g\" % metrics.accuracy_score(y, yhat))\nprint(\"AUC Score: %f\" % metrics.roc_auc_score(y, yhat))    ", "execution_count": 33, "outputs": [{"output_type": "stream", "text": "\nModel Report\nAccuracy : 0.9849\nAUC Score: 0.895927\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Place the newly scored records into deciles."}, {"metadata": {}, "cell_type": "code", "source": "#assign a tiny random number to the probability to brake ties\ndfx['bingbong'] = (np.random.randint(0, 100, dfx.shape[0]))/100000000000000000\n\ndfx['P_BUY']=(dfx['P_BUY']+dfx['bingbong'])\n\n#Create deciles based on P_BUY\ndfx['DECILE'] = pd.qcut(dfx['P_BUY'], 10, labels=np.arange(100, 0, -10))\n", "execution_count": 34, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Select the top 3 deciles."}, {"metadata": {}, "cell_type": "code", "source": "dfx['DECILE']=dfx['DECILE'].astype(int)\ndf_x=dfx[dfx['DECILE']<=30]", "execution_count": 35, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_count = pd.DataFrame(df_x.groupby(['MARKET'])['BUY'].count())", "execution_count": 36, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_count", "execution_count": 37, "outputs": [{"output_type": "execute_result", "execution_count": 37, "data": {"text/plain": "           BUY\nMARKET        \nKarma    15702\nYarnaby  17631\nYuba     15526", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BUY</th>\n    </tr>\n    <tr>\n      <th>MARKET</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Karma</th>\n      <td>15702</td>\n    </tr>\n    <tr>\n      <th>Yarnaby</th>\n      <td>17631</td>\n    </tr>\n    <tr>\n      <th>Yuba</th>\n      <td>15526</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Note the distribution of the top 3 deciles across the various markets.  The number of records in each market is about the same, roughly anyway.  \n\nYarnaby has a few thousand more than either Karma or Yuba.  There are a few reasons this could happen.  One, it is possible that Yarnaby is a better market than the other two, and as BBAA starts selling in Yarnaby, they will realize better sales than in their existing markets.  While that is possible, I kind of doubt that to be the case here.  An issue with this method is that we rely on sampling to balance the conversion rates between the markets.  Sample, as you know, comes with sampling error.  My guess is that if we select a different random selection of records, the results might look different.\n\nAgain, we don't know for sure because we don't know the actual conversion rate in Yarnaby.  We assume that it will be the same as it is in the other two markets.\n\nLike the previous example above, this method lends itself to real-time scoring, given that the model output is comparable across the three markets.   \nThis method also lends itself to situations when you need to resample your data anyway.  For example, if you have a tiny (less than 1%) purchase rate, you may decide to balance or resample your data anyway."}, {"metadata": {}, "cell_type": "markdown", "source": "##  3.0 Historical Discrimination <a id=\"30\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "Ed and Son's Dog Bathing Incorporated is dedicated to cleaning canines in the tri-state area.  Ed started the business thirty years ago and added his son as a business partner twelve years ago.  Ed, getting up in age, is now looking to retire.  Once he retires, his son Elmo will continue the family business for another generation.\n\nEd's business includes all breeds of dogs except for one, the Basset Hound.  Ed hates basset hounds.  He thinks they are stubborn, their ears are too long, and they insist on stealing humans' food.  He just doesn't like basset hounds.  In fact, you could say Ed has actively discriminated against basset hounds in his thirty years of dog washing, refusing to do business with them.\n\nNow that Ed is heading into retirement, Elmo is looking to expand the business.  One easy way to grow the market is to start bathing basset hounds.  Another way Elmo plans to grow the company is to use a machine learning model to identify prospects for dog washing in the tri-state area.  Elmo will use historical data to understand who purchased dog washes in the past and then use this learning to find new prospects.  Elmo plans to apply a model to the universe of candidates, then call the top 30% to see if they want a dog wash."}, {"metadata": {}, "cell_type": "markdown", "source": "##### 3.1 Pull in the data from GitHub.<a id=\"31\"></a>"}, {"metadata": {}, "cell_type": "code", "source": "!rm breeds.csv\n!wget https://raw.githubusercontent.com/shadgriffin/ml_bias/main/breeds.csv\n    \ndf = pd.read_csv(\"breeds.csv\", sep=\",\", header=0)", "execution_count": 38, "outputs": [{"output_type": "stream", "text": "rm: cannot remove 'breeds.csv': No such file or directory\n--2022-01-07 22:39:26--  https://raw.githubusercontent.com/shadgriffin/ml_bias/main/breeds.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4455824 (4.2M) [text/plain]\nSaving to: \u2018breeds.csv\u2019\n\nbreeds.csv          100%[===================>]   4.25M  --.-KB/s    in 0.04s   \n\n2022-01-07 22:39:26 (121 MB/s) - \u2018breeds.csv\u2019 saved [4455824/4455824]\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "##### 3.2 Examine the results <a id=\"32\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "The dataset is pretty simple.  Again, it is 100% fake. \n\nCUST_ID -- Unique Identifier of the prospect\n\nTHEPREDICTOR -- This is the primary independent variable of the model.\n\nBREED -- The breed of the prospect.  It can be Basset Hound or Other.\n\nBUY -- The dependent variable.  A '1' means the prospect purchased carpet cleaning in the past; a '0' means they did not."}, {"metadata": {}, "cell_type": "code", "source": "df.head()", "execution_count": 39, "outputs": [{"output_type": "execute_result", "execution_count": 39, "data": {"text/plain": "    CUST_ID  THEPREDICTOR         BREED  BUY\n0  10000007    -97.248989  Basset Hound    0\n1  10000021    -98.154992  Basset Hound    0\n2  10000028    -98.442994  Basset Hound    0\n3  10000036    -98.415993  Basset Hound    0\n4  10000037    -99.681999  Basset Hound    0", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CUST_ID</th>\n      <th>THEPREDICTOR</th>\n      <th>BREED</th>\n      <th>BUY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10000007</td>\n      <td>-97.248989</td>\n      <td>Basset Hound</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10000021</td>\n      <td>-98.154992</td>\n      <td>Basset Hound</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10000028</td>\n      <td>-98.442994</td>\n      <td>Basset Hound</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10000036</td>\n      <td>-98.415993</td>\n      <td>Basset Hound</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10000037</td>\n      <td>-99.681999</td>\n      <td>Basset Hound</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Looking at the historical data, it is evident that basset hounds have not been a priority.  There is only fourteen basset hound customers in the historical data.  More and likely a mixed-breeds."}, {"metadata": {}, "cell_type": "code", "source": "df['wookie']=1\n\ndf_count = pd.DataFrame(df.groupby(['BREED','wookie'])['BUY'].count())\ndf_sum = pd.DataFrame(df.groupby(['BREED','wookie'])['BUY'].sum())\ndf_predictor = pd.DataFrame(df.groupby(['BREED','wookie'])['THEPREDICTOR'].mean())\ndf_total = pd.DataFrame(df.groupby(['wookie'])['BUY'].count())\n\ndf_count=df_count.rename(columns={\"BUY\": \"Total Prospects\"})\ndf_sum=df_sum.rename(columns={\"BUY\": \"Buyers\"})\ndf_total=df_total.rename(columns={\"BUY\": \"All Breeds\"})\n\ndf_out = pd.concat([ df_count,df_sum,df_predictor], axis=1)\n\ndf_total=df_total.reset_index()\ndf_out=df_out.reset_index()\n\ndf_out = pd.merge(df_out, df_total, on=\"wookie\")\ndf_out['Conversion Rate']=df_out['Buyers']/df_out['Total Prospects']\ndf_out['Percent of Prospects']=df_out['Total Prospects']/df_out['All Breeds']\ndf_out['Conversion Rate']=df_out['Buyers']/df_out['Total Prospects']\n\ndf_out=df_out.drop(columns='wookie')\ndf_out", "execution_count": 40, "outputs": [{"output_type": "execute_result", "execution_count": 40, "data": {"text/plain": "          BREED  Total Prospects  Buyers  THEPREDICTOR  All Breeds  \\\n0  Basset Hound            13663      14    -98.295280      123903   \n1         Other           110240    6438    597.421517      123903   \n\n   Conversion Rate  Percent of Prospects  \n0         0.001025              0.110272  \n1         0.058400              0.889728  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BREED</th>\n      <th>Total Prospects</th>\n      <th>Buyers</th>\n      <th>THEPREDICTOR</th>\n      <th>All Breeds</th>\n      <th>Conversion Rate</th>\n      <th>Percent of Prospects</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Basset Hound</td>\n      <td>13663</td>\n      <td>14</td>\n      <td>-98.295280</td>\n      <td>123903</td>\n      <td>0.001025</td>\n      <td>0.110272</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Other</td>\n      <td>110240</td>\n      <td>6438</td>\n      <td>597.421517</td>\n      <td>123903</td>\n      <td>0.058400</td>\n      <td>0.889728</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "##### 3.3 Build a Machine Learning Model to predict the best prospects.<a id=\"33\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "Create a dummy variable to identify Basset Hounds."}, {"metadata": {}, "cell_type": "code", "source": "df['BASSET'] = np.where(((df.BREED == 'Basset Hound')), 1, 0)", "execution_count": 41, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Define the independent and dependent variables."}, {"metadata": {}, "cell_type": "code", "source": "X=pd.DataFrame(df[['THEPREDICTOR']])\ny=pd.DataFrame(df['BUY'])", "execution_count": 42, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Define and fit the model."}, {"metadata": {}, "cell_type": "code", "source": "xgb21 = XGBClassifier(objective = 'binary:logistic',use_label_encoder=False);\nxgb21.fit(X,y,eval_metric='auc')", "execution_count": 43, "outputs": [{"output_type": "execute_result", "execution_count": 43, "data": {"text/plain": "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=100, n_jobs=56, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', use_label_encoder=False,\n              validate_parameters=1, verbosity=None)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "#Score the original data with the model\nphat= xgb21.predict_proba(X);\n#Put the results in a dataframe\ndf_phat=pd.DataFrame(phat)\n#add Variable lablels\ndf_phat=df_phat.rename(columns={0: \"P_NOBUY\", 1:\"P_BUY\"})\n#append values to the original data\ndfx = pd.concat([df, df_phat], axis=1)", "execution_count": 44, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Evaluate the accuracy of the model"}, {"metadata": {}, "cell_type": "code", "source": "\nyhat= xgb21.predict(X);\n\n\ndf_yhat=pd.DataFrame(yhat)\nprint(\"\\nModel Report\")\nprint(\"Accuracy : %.4g\" % metrics.accuracy_score(y, yhat))\nprint(\"AUC Score: %f\" % metrics.roc_auc_score(y, yhat))    ", "execution_count": 45, "outputs": [{"output_type": "stream", "text": "\nModel Report\nAccuracy : 0.9846\nAUC Score: 0.926639\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Place the records in Deciles"}, {"metadata": {}, "cell_type": "code", "source": "#Add a tiny random number to the probability to break ties\ndfx['bingbong'] = (np.random.randint(0, 100, df.shape[0]))/100000000000000000\n\ndfx['P_BUY']=(dfx['P_BUY']+dfx['bingbong'])\n\n#Create deciles based on P_BUY\ndfx['DECILE'] = pd.qcut(dfx['P_BUY'], 10, labels=np.arange(100, 0, -10))\n", "execution_count": 46, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Evaluate the deciles.  The model does an excellent job of identifying prospects, but note that all the basset hounds are in the bottom decile."}, {"metadata": {}, "cell_type": "code", "source": "df_sum = pd.DataFrame(dfx.groupby(['DECILE'])['BUY'].sum())\ndf_count = pd.DataFrame(dfx.groupby(['DECILE'])['BUY'].count())\ndf_mean = pd.DataFrame(dfx.groupby(['DECILE'])['BUY'].mean())\ndf_phat = pd.DataFrame(dfx.groupby(['DECILE'])['P_BUY'].mean())\ndfx['BASSET'] = np.where(((dfx.BREED == 'Basset Hound')), 1, 0)\ndf_yarnaby = pd.DataFrame(dfx.groupby(['DECILE'])['BASSET'].sum())\n\ndfx.head()\n\n\ndf_out = pd.concat([df_mean, df_count,df_sum,df_yarnaby,df_phat], axis=1)\ndf_out", "execution_count": 47, "outputs": [{"output_type": "execute_result", "execution_count": 47, "data": {"text/plain": "             BUY    BUY   BUY  BASSET     P_BUY\nDECILE                                         \n100     0.000000  12395     0   12395  0.000399\n90      0.001211  12387    15    1025  0.003750\n80      0.003309  12391    41      21  0.005289\n70      0.005166  12389    64      15  0.006207\n60      0.007099  12396    88      47  0.007244\n50      0.007911  12388    98      11  0.008111\n40      0.009037  12394   112       5  0.009018\n30      0.011387  12382   141       0  0.010288\n20      0.015577  12390   193       1  0.013065\n10      0.460011  12391  5700     143  0.457366", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BUY</th>\n      <th>BUY</th>\n      <th>BUY</th>\n      <th>BASSET</th>\n      <th>P_BUY</th>\n    </tr>\n    <tr>\n      <th>DECILE</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>100</th>\n      <td>0.000000</td>\n      <td>12395</td>\n      <td>0</td>\n      <td>12395</td>\n      <td>0.000399</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>0.001211</td>\n      <td>12387</td>\n      <td>15</td>\n      <td>1025</td>\n      <td>0.003750</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>0.003309</td>\n      <td>12391</td>\n      <td>41</td>\n      <td>21</td>\n      <td>0.005289</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>0.005166</td>\n      <td>12389</td>\n      <td>64</td>\n      <td>15</td>\n      <td>0.006207</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>0.007099</td>\n      <td>12396</td>\n      <td>88</td>\n      <td>47</td>\n      <td>0.007244</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>0.007911</td>\n      <td>12388</td>\n      <td>98</td>\n      <td>11</td>\n      <td>0.008111</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.009037</td>\n      <td>12394</td>\n      <td>112</td>\n      <td>5</td>\n      <td>0.009018</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.011387</td>\n      <td>12382</td>\n      <td>141</td>\n      <td>0</td>\n      <td>0.010288</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.015577</td>\n      <td>12390</td>\n      <td>193</td>\n      <td>1</td>\n      <td>0.013065</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.460011</td>\n      <td>12391</td>\n      <td>5700</td>\n      <td>143</td>\n      <td>0.457366</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Select on the top 3 deciles"}, {"metadata": {}, "cell_type": "code", "source": "dfx['DECILE']=dfx['DECILE'].astype(int)\ndf_x=dfx[dfx['DECILE']<=30]", "execution_count": 48, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Evaluate the distribution of top records by breed."}, {"metadata": {}, "cell_type": "markdown", "source": "There are not many basset hounds in the top three deciles.  Again, hopefully, the problem is apparent.  The machine learning model doesn't know Ed actively discriminated against basset hounds.  It has no way to know this.  If you use the historical data to identify the best prospects, basset hounds will never make the cut.  This isn't because they are perpetually clean.  It is because they were discriminated against in the historical data."}, {"metadata": {}, "cell_type": "code", "source": "df_count = pd.DataFrame(df_x.groupby(['BREED'])['THEPREDICTOR'].count())\ndf_count", "execution_count": 49, "outputs": [{"output_type": "execute_result", "execution_count": 49, "data": {"text/plain": "              THEPREDICTOR\nBREED                     \nBasset Hound           144\nOther                37019", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>THEPREDICTOR</th>\n    </tr>\n    <tr>\n      <th>BREED</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Basset Hound</th>\n      <td>144</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>37019</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "##### 3.4 Monkey with the Output Data. <a id=\"34\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "Previously, we examined ways of dealing with \"Machine Learning Bias\" or situations where your historical data does not reflect your current objectives.  We saw how you could alter the data or the sample to bring the machine learning model closer to what you hope to achieve.  Another approach is to monkey with the output data.  In this case, you build the machine learning model on the original data but then change how you use it.  Below is a concrete example of how to do this."}, {"metadata": {}, "cell_type": "markdown", "source": "Take the data from the model and separate each breed into a different data frame."}, {"metadata": {}, "cell_type": "code", "source": "df_boogie=dfx[['CUST_ID','P_BUY','BREED','BUY']]\n\ndf_b=df_boogie[df_boogie['BREED']=='Basset Hound'].copy()\ndf_e=df_boogie[df_boogie['BREED']!='Basset Hound'].copy()\n", "execution_count": 50, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Remember, the goal is to contact 30% of the prospects. Note that Basset Hounds represent about 11% of the total population. Here is how to ensure that Bassets reflect about 11% of the final targeted prospect file."}, {"metadata": {}, "cell_type": "markdown", "source": "Assign deciles within each breed."}, {"metadata": {}, "cell_type": "code", "source": "df_b['DECILE'] = pd.qcut(df_b['P_BUY'], 10, labels=np.arange(100, 0, -10))\ndf_e['DECILE'] = pd.qcut(df_e['P_BUY'], 10, labels=np.arange(100, 0, -10))\n\ndf_b['DECILE']=df_b['DECILE'].astype(int)\ndf_e['DECILE']=df_e['DECILE'].astype(int)\n\n", "execution_count": 51, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Append the records back together into one dataframe."}, {"metadata": {}, "cell_type": "code", "source": "df_boogie = pd.concat([df_b, df_e])", "execution_count": 52, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Select the top 3 deciles."}, {"metadata": {}, "cell_type": "code", "source": "df_boogie=df_boogie[df_boogie['DECILE']<=30]", "execution_count": 53, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Examine the results."}, {"metadata": {}, "cell_type": "code", "source": "df_count = pd.DataFrame(df_boogie.groupby(['BREED'])['P_BUY'].count())\ndf_count", "execution_count": 54, "outputs": [{"output_type": "execute_result", "execution_count": 54, "data": {"text/plain": "              P_BUY\nBREED              \nBasset Hound   4090\nOther         33070", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>P_BUY</th>\n    </tr>\n    <tr>\n      <th>BREED</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Basset Hound</th>\n      <td>4090</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>33070</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Using the method above, we can ensure that Basset Hounds are represented proportionally to their presence in the original prospect file, even though they have experienced historical discrimination. \n\nWhat we are doing in this latest example is establishing a quota.  The methodology above will ensure that the at-risk group is represented proportionally to their presence in the population.  You could easily change this so that Basset Hounds represent 1%, 30%, or 50% of the targeted group.  Again, it just depends on your objective.\n\nMonkeying with the output is another way to correct \"Machine learning Bias\".  This method could make sense if you are looking for exact numbers.  For example, you want 20% of the final targeted list to be basset hounds.  This method also lends itself to batch deployments, given that you have to aggregate the data to know how to adjust it."}, {"metadata": {}, "cell_type": "markdown", "source": "## 4.0 Conclusion <a id=\"40\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "In this article, I have laid out scenarios where building a machine learning model on your historical data will not meet your current business objects. Many call this \"Machine Learning Bias\".   If you encounter \"Machine Learning Bias,\" you have a problem with your data, and there is no sure-fire way to fix data that is missing something you need. This article presented three ways to deal with \"Machine Learning Bias\". Hopefully, I made clear how you can use these techniques to minimize the impact of this kind of problem.\n\nAs far as which of the three techniques is best, it is tough to say. It does depend on the specific problem you are looking to solve. I would say that generally, if you are looking to deploy your model in real-time via an application, then the first two mentioned in the article are probably the best. Those being Monkeying with the scoring data and Monkeying with the sample. The third (Monkeying with the output data) is probably the best if you are doing a batch deployment. Again, it depends on your objectives and the size of the gap n the historical data.   I'd suggest a bit of trial and error with all three methods.  \n\nI hope this was helpful."}, {"metadata": {}, "cell_type": "markdown", "source": "### Author"}, {"metadata": {}, "cell_type": "markdown", "source": "**Shad Griffin**, is a Data Scientist headquartered in a Treehouse sitting on top of Idiot's Hill in Denton, Texas"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.8", "language": "python"}, "language_info": {"name": "python", "version": "3.8.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}